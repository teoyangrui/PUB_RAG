{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012107b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import json\n",
    "from typing import List \n",
    "from collections import defaultdict\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import base64\n",
    "import pickle\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ff0503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load configurations for later\n",
    "#get openAI API key\n",
    "load_dotenv('.env')\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "#The 2 code of practice pdf documents\n",
    "pdf_files = [\n",
    "    \"./data/Code-of-Practice-on-Surface-Water-Drainage.pdf\",\n",
    "    \"./data/Code of Practice on Sewerage and Sanitary Works 3rd Edition  Mar 2025.pdf\"\n",
    "]\n",
    "#name of local vector store\n",
    "db_dir = \"chroma_parent_child\"\n",
    "#name of directory for extracted figures\n",
    "OUTPUT_IMAGE_DIR = \"auto_figures\"\n",
    "os.makedirs(OUTPUT_IMAGE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734fb6c8",
   "metadata": {},
   "source": [
    "### Step 1: Extract text from PDF and Create Parent and Child Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eeb3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#regular expression to identify part of the document that are annexes so that after retrival, \n",
    "#if the annex was mentioned in the reference chunk, it will be retrieved to use as additional context \n",
    "ref_regex = re.compile(\n",
    "    r\"\\b(?:annex|appendix)\\s+[a-z\\d]+\\b\"\n",
    "    r\"|\\bdrawing\\s+no\\.?\\s*\\d+\\b\"\n",
    "    r\"|\\bfigure\\s+\\d+(?:\\.\\d+)*\\b\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Normalization and extraction function\n",
    "# references in documents come in many different forms and need to be normalized. e.g. annex K, AnnexK, Annex  K -> annex k\n",
    "def normalize_label(label: str) -> str: \n",
    "    label = label.lower().replace(\"\\n\", \" \")\n",
    "    label = re.sub(r\"\\s+\", \" \", label.strip())\n",
    "    label = re.sub(r\"(\\bno)\\s*(\\d)\", r\"\\1 \\2\", label)  \n",
    "    return label\n",
    "\n",
    "def extract_annex_refs(text: str):\n",
    "    matches = ref_regex.findall(text)\n",
    "    return list(set([normalize_label(m) for m in matches]))\n",
    "\n",
    "#split the document and obtain chunks\n",
    "all_chunks = []\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "for path in pdf_files:\n",
    "    loader = PyPDFLoader(path)\n",
    "    page_docs = loader.load()  # one Document per PDF page as parent\n",
    "    basename = os.path.basename(path)\n",
    "\n",
    "    for page_doc in page_docs:\n",
    "        #get page number and document name to cite as source later\n",
    "        page_num = page_doc.metadata.get(\"page\", None) \n",
    "        page_doc.metadata[\"source\"] = basename\n",
    "\n",
    "        # split each page into child chunks; carry forward metadata\n",
    "        chunks = child_splitter.split_documents([page_doc])\n",
    "        for c in chunks:\n",
    "            c.metadata[\"source\"] = basename\n",
    "            if page_num is not None:\n",
    "                c.metadata[\"page\"] = int(page_num)\n",
    "            c.metadata[\"parent_preview\"] = page_doc.page_content[:500]\n",
    "            #extract any mention of references and store as metadata\n",
    "            c.metadata[\"annex_refs\"] = extract_annex_refs(c.page_content) \n",
    "            all_chunks.append(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9377dd6",
   "metadata": {},
   "source": [
    "### Step 2: Parse the document again to extract references with images and describe them with words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cb2b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optical character recognition to convert pages that are scanned drawings.\n",
    "def ocr_page(pdf_path, page_num):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pix = doc[page_num - 1].get_pixmap(dpi=300)\n",
    "    img = Image.open(io.BytesIO(pix.tobytes(\"png\")))\n",
    "    return pytesseract.image_to_string(img)\n",
    "\n",
    "# as reference diagrams e.g. figure 1 may be mentioned multiple times, for the purpose of this project\n",
    "#we assume that the last mention of the figure will the page where the image/drawing is.\n",
    "#thats where it will be detected and extracted.\n",
    "def detect_last_occurrences(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    last_seen = {}  # {norm_label: {canonical, page}}\n",
    "    unlabelled_pages = set()\n",
    "\n",
    "    for page_index, page in enumerate(doc):\n",
    "        page_num = page_index + 1\n",
    "        text_layer = page.get_text(\"text\")\n",
    "        ocr_text = \"\"\n",
    "        if not text_layer.strip() and page.get_images(full=True):\n",
    "            ocr_text = ocr_page(pdf_path, page_num)\n",
    "\n",
    "        merged_text = (text_layer or \"\") + \"\\n\" + ocr_text\n",
    "        matches = ref_regex.findall(merged_text)\n",
    "\n",
    "        if matches:\n",
    "            for match in matches:\n",
    "                norm = normalize_label(match)\n",
    "                # Only overwrite if this is later in the doc\n",
    "                if norm not in last_seen or page_num > last_seen[norm][\"page\"]:\n",
    "                    last_seen[norm] = {\n",
    "                        \"canonical\": match.strip(),\n",
    "                        \"page\": page_num\n",
    "                    }\n",
    "        elif page.get_images(full=True):\n",
    "            unlabelled_pages.add(page_num)\n",
    "\n",
    "    return last_seen, unlabelled_pages\n",
    "\n",
    "#Extract image in pdf in a format for the vision models to describe\n",
    "def extract_images_from_page(pdf_path, page_number, output_dir=OUTPUT_IMAGE_DIR):\n",
    "    pdf_doc = fitz.open(pdf_path)\n",
    "    page = pdf_doc[page_number - 1]\n",
    "    image_paths = []\n",
    "    for img_index, img in enumerate(page.get_images(full=True)):\n",
    "        xref = img[0]\n",
    "        pix = fitz.Pixmap(pdf_doc, xref)\n",
    "        if pix.n < 5:  # RGB\n",
    "            img_path = os.path.join(output_dir, f\"{os.path.basename(pdf_path)}_p{page_number}_{img_index}.png\")\n",
    "            pix.save(img_path)\n",
    "        else:  # CMYK â†’ RGB\n",
    "            pix = fitz.Pixmap(fitz.csRGB, pix)\n",
    "            img_path = os.path.join(output_dir, f\"{os.path.basename(pdf_path)}_p{page_number}_{img_index}.png\")\n",
    "            pix.save(img_path)\n",
    "        image_paths.append(img_path)\n",
    "        pix = None\n",
    "    return image_paths\n",
    "\n",
    "#both caption and understanding of diagram/words on images will be used as text to describe that image\n",
    "# use blip to get overall image captioning\n",
    "def blip_describe_image(image_path):\n",
    "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    output = model.generate(**inputs)\n",
    "    return processor.decode(output[0], skip_special_tokens=True)\n",
    "# use gpt vision to convert understanding of any figures etc into words\n",
    "def gpt_vision_describe_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        image_base64 = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "    client = OpenAI(api_key=API_KEY)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"Describe this engineering diagram in detail.\"},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{image_base64}\"}}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=750\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# bring the fuctions together to label and describe the references and annexes\n",
    "def build_figure_docs(pdf_path):\n",
    "    label_map = {} #map reference name to diagram/annex/figure description. when annex is detected in retrieved chunk, \n",
    "    #the related reference will be retrieved using this label_map\n",
    "    figure_docs = []\n",
    "    #identify last seen pages\n",
    "    last_seen, unlabelled_pages = detect_last_occurrences(pdf_path)\n",
    "\n",
    "    # Labelled diagrams\n",
    "    for norm_label, info in last_seen.items():\n",
    "        label_map[norm_label] = info[\"canonical\"]\n",
    "        img_paths = extract_images_from_page(pdf_path, info[\"page\"])\n",
    "        for img_path in img_paths: #use image models to describe image\n",
    "            blip_caption = blip_describe_image(img_path)\n",
    "            gpt_caption = gpt_vision_describe_image(img_path)\n",
    "            caption_combined = f\"{info['canonical']}\\n[BLIP] {blip_caption}\\n[GPT-4V] {gpt_caption}\"\n",
    "            #append information to figure_docs\n",
    "            figure_docs.append(Document(\n",
    "                page_content=f\"[DIAGRAM CAPTION]\\n{caption_combined}\",\n",
    "                metadata={\n",
    "                    \"source\": os.path.basename(pdf_path),\n",
    "                    \"is_diagram\": True,\n",
    "                    \"figure_label\": info[\"canonical\"],\n",
    "                    \"figure_label_norm\": norm_label,\n",
    "                    \"page_number\": info[\"page\"],\n",
    "                    \"image_path\": img_path\n",
    "                }\n",
    "            ))\n",
    "\n",
    "    # Do the same for unlabelled diagrams\n",
    "    for page_num in unlabelled_pages:\n",
    "        img_paths = extract_images_from_page(pdf_path, page_num)\n",
    "        for img_path in img_paths:\n",
    "            blip_caption = blip_describe_image(img_path)\n",
    "            gpt_caption = gpt_vision_describe_image(img_path)\n",
    "            label = f\"UNLABELLED_DIAGRAM_p{page_num}\"\n",
    "            caption_combined = f\"{label}\\n[BLIP] {blip_caption}\\n[GPT-4V] {gpt_caption}\"\n",
    "            figure_docs.append(Document(\n",
    "                page_content=f\"[DIAGRAM CAPTION]\\n{caption_combined}\",\n",
    "                metadata={\n",
    "                    \"source\": os.path.basename(pdf_path),\n",
    "                    \"is_diagram\": True,\n",
    "                    \"figure_label\": label,\n",
    "                    \"figure_label_norm\": label.lower(),\n",
    "                    \"page_number\": page_num,\n",
    "                    \"image_path\": img_path\n",
    "                }\n",
    "            ))\n",
    "\n",
    "    return figure_docs, label_map\n",
    "\n",
    "# Build figure docs & label map (skip if pickle exists) \n",
    "all_figure_docs = []\n",
    "global_label_map = {}\n",
    "pickle_file = \"figure_docs.pkl\"\n",
    "\n",
    "if os.path.exists(pickle_file):\n",
    "    print(f\"âœ… Skipping figure processing â€” loaded from {pickle_file}\")\n",
    "    with open(pickle_file, \"rb\") as f:\n",
    "        all_figure_docs = pickle.load(f)\n",
    "    if os.path.exists(\"label_map.json\"):\n",
    "        with open(\"label_map.json\", \"r\") as f:\n",
    "            global_label_map = json.load(f)\n",
    "else:\n",
    "    for pdf_path in pdf_files:\n",
    "        figs, label_map = build_figure_docs(pdf_path)\n",
    "        all_figure_docs.extend(figs)\n",
    "        global_label_map.update(label_map)\n",
    "    with open(pickle_file, \"wb\") as f:\n",
    "        pickle.dump(all_figure_docs, f)\n",
    "    with open(\"label_map.json\", \"w\") as f:\n",
    "        json.dump(global_label_map, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d38872c",
   "metadata": {},
   "source": [
    "### Step 3 Processing of data and creation of vectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cc5fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _stringify_primitive(v):\n",
    "    # allow primitives only\n",
    "    return v if isinstance(v, (str, int, float, bool)) or v is None else str(v)\n",
    "\n",
    "def filter_complex_metadata(docs: List[Document]) -> List[Document]:\n",
    "    keep_keys = {\n",
    "        \"source\", \"page\", \"parent_preview\",\n",
    "        \"annex_refs\", \"is_diagram\", \"figure_label\", \"figure_label_norm\",\n",
    "        \"page_number\", \"image_path\"\n",
    "    }\n",
    "    #from typing imported list at the top\n",
    "    filtered: List[Document] = []\n",
    "    for d in docs:\n",
    "        md = d.metadata or {}\n",
    "        md_simple = {k: md[k] for k in keep_keys if k in md} # keep required keys\n",
    "\n",
    "        # normalize 'page' (figures may use 'page_number')\n",
    "        if \"page\" not in md_simple and \"page_number\" in md_simple:\n",
    "            try:\n",
    "                md_simple[\"page\"] = int(md_simple[\"page_number\"])\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        #Chroma disallows list metadata. Convert annex_refs (list) â†’ CSV string; drop if empty.\n",
    "        if \"annex_refs\" in md_simple:\n",
    "            v = md_simple[\"annex_refs\"]\n",
    "            if isinstance(v, (list, tuple, set)):\n",
    "                v = [str(x) for x in v if str(x).strip()]\n",
    "                if v:\n",
    "                    md_simple[\"annex_refs_csv\"] = \"; \".join(v)  # new scalar field\n",
    "                # remove the list field entirely\n",
    "                md_simple.pop(\"annex_refs\", None)\n",
    "\n",
    "        #cap preview to keep metadata small\n",
    "        if \"parent_preview\" in md_simple and isinstance(md_simple[\"parent_preview\"], str):\n",
    "            md_simple[\"parent_preview\"] = md_simple[\"parent_preview\"][:500]\n",
    "\n",
    "        # stringify everything to primitives\n",
    "        md_simple = {k: _stringify_primitive(v) for k, v in md_simple.items()}\n",
    "\n",
    "        filtered.append(Document(page_content=d.page_content, metadata=md_simple))\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9936e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create vector db\n",
    "# Merge text chunks with figure captions and filter metadata\n",
    "all_chunks = filter_complex_metadata(all_chunks + all_figure_docs)\n",
    "\n",
    "#use openai's embeddings\n",
    "embedding = OpenAIEmbeddings(openai_api_key=API_KEY)\n",
    "\n",
    "#create vector db using chroma\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=all_chunks,\n",
    "    embedding=embedding,\n",
    "    persist_directory=db_dir\n",
    ")\n",
    "vectordb.persist()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
